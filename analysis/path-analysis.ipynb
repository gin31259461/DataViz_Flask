{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path analysis for data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Tuple, Type\n",
    "\n",
    "sys.path.append(f\"{Path.cwd().parent.absolute()}/\")\n",
    "from setup import setup\n",
    "\n",
    "db = setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OID = 139\n",
    "skip_features = []\n",
    "datetime_format = \"\"\n",
    "# TODO: Concept hierarchy is needed to implement\n",
    "concept_hierarchy = {\"教育程度類別\": [\"其他\", \"高中高職\", \"專科\", \"大學\", \"碩士\", \"博士\"], \"產業別\": [\"其他\", \"百貨\", \"文教康樂\", \"行\", \"住\", \"衣\", \"食\"]}\n",
    "target = \"信用卡交易金額[新台幣]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "query = text(f\"SELECT * FROM [RawDB].[dbo].[D{OID}]\")\n",
    "data = db.execute(query)\n",
    "\n",
    "query = text(\"SELECT * FROM [DV].[dbo].[Object] where OID = :OID\")\n",
    "object_table = db.execute(query, OID=OID).fetchall()\n",
    "\n",
    "df = pd.DataFrame(data.fetchall())\n",
    "copy_of_df = df.copy()\n",
    "\n",
    "data_name = pd.DataFrame(object_table)[\"CName\"][0]\n",
    "print(f\"Data name: {data_name}\")\n",
    "\n",
    "column_names = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical column handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_mapping = {}\n",
    "\n",
    "numerical_column_tuple: Tuple[str, int, int] = []\n",
    "for col in column_names:\n",
    "    column_ratio = len(df[col].unique()) / df[col].count()\n",
    "    is_categorical_column = df[col].dtype == \"object\" or df[col].dtype == \"category\" or column_ratio < 0.01\n",
    "    is_numerical_column = df[col].dtype == \"int64\" and not is_categorical_column\n",
    "    if is_numerical_column:\n",
    "        numerical_column_tuple.append([col, df[col].min(), df[col].max()])\n",
    "\n",
    "numerical_column_tuple\n",
    "\n",
    "\n",
    "for col in concept_hierarchy:\n",
    "    df[col] = pd.Series(pd.Categorical(values=df[col], categories=concept_hierarchy[col], ordered=True))\n",
    "\n",
    "\n",
    "quantile_numeric = []\n",
    "\n",
    "for tuple in numerical_column_tuple:\n",
    "    col = tuple[0]\n",
    "    discrete_bin_num = 3\n",
    "    quantile_labels = [\"low\", \"middle\", \"high\"]\n",
    "    quantile = pd.qcut(df[col], q=[0, 0.1575, 0.8425, 1])\n",
    "    quantile_numeric.append(col)\n",
    "    df[col] = pd.qcut(df[col], q=[0, 0.1575, 0.8425, 1], labels=quantile_labels)\n",
    "    quantile_mapping[col] = pd.Series(data=quantile.unique().tolist(), index=df[col].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime column handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from math import log\n",
    "from pandas import DatetimeIndex\n",
    "\n",
    "\n",
    "datetime_format_list = [\"%Y-%m-%d\", \"%Y-%m\", \"%Y%m%d\", \"%Y%m\", \"%Y\"]\n",
    "datetime_column_tuple: Tuple[str, Type[pd.Timestamp], Type[pd.Timestamp]] = []\n",
    "\n",
    "if len(datetime_format) != 0:\n",
    "    datetime_format_list.insert(0, datetime_format)\n",
    "\n",
    "\n",
    "# to_datetime reference: https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n",
    "\n",
    "for col in column_names:\n",
    "    is_datetime = df[col].dtype == \"datetime64[ns]\"\n",
    "    if is_datetime:\n",
    "        datetime_column_tuple.append(\n",
    "            [\n",
    "                col,\n",
    "                df[col].min(),\n",
    "                df[col].max(),\n",
    "            ]\n",
    "        )\n",
    "        continue\n",
    "    for test_format in datetime_format_list:\n",
    "        is_numeric = df[col].dtype == \"int64\"\n",
    "        is_datetime = is_numeric and (\n",
    "            True\n",
    "            not in pd.to_datetime(arg=df[col].astype(\"str\"), format=test_format, errors=\"coerce\")\n",
    "            .isna()\n",
    "            .value_counts()\n",
    "            .index.to_list()\n",
    "        )\n",
    "        if is_datetime:\n",
    "            parsed_datetime = pd.to_datetime(arg=df[col], format=test_format, errors=\"coerce\")\n",
    "            df[col] = parsed_datetime\n",
    "            datetime_column_tuple.append(\n",
    "                [\n",
    "                    col,\n",
    "                    parsed_datetime.min(),\n",
    "                    parsed_datetime.max(),\n",
    "                ]\n",
    "            )\n",
    "            break\n",
    "\n",
    "datetime_column_tuple\n",
    "\n",
    "# Quantize datetime problem reference:\n",
    "# https://stackoverflow.com/questions/43500894/pandas-pd-cut-binning-datetime-column-series\n",
    "datetime_columns = []\n",
    "\n",
    "# ! testing ######################\n",
    "\n",
    "\n",
    "def count_feature_entropy(feature_name: str, target_name: str, feature_values: list, target_values):\n",
    "    part_df = pd.DataFrame(columns=[feature_name, target_name])\n",
    "    part_df[feature_name] = feature_values\n",
    "    part_df[target_name] = target_values\n",
    "\n",
    "    class_count = len(part_df[target_name].unique())\n",
    "    total_rows = len(part_df[target_name])\n",
    "    weight_average = 0\n",
    "    accumulation = 0\n",
    "\n",
    "    # FIXME: for nan value\n",
    "    for value in part_df[feature_name].dropna().unique().tolist():\n",
    "        value_df = part_df.loc[part_df[feature_name] == value]\n",
    "        target_values = value_df[target_name].value_counts().values.tolist()\n",
    "        H = [\n",
    "            -(target_value / total_rows) * log(target_value / total_rows, class_count) for target_value in target_values\n",
    "        ]\n",
    "        entropy = sum(H)\n",
    "        accumulation += 1\n",
    "        weight_average += entropy\n",
    "\n",
    "    weight_average /= accumulation\n",
    "    return weight_average\n",
    "\n",
    "\n",
    "def fill_label(freq: str, datetime_manifest: Type[DatetimeIndex]):\n",
    "    labels: list[str] = []\n",
    "\n",
    "    match freq:\n",
    "        case \"year\":\n",
    "            labels = [str(year) for year in list(datetime_manifest.year)]\n",
    "            if len(labels) >= 0:\n",
    "                labels.pop()\n",
    "        case \"quarter\":\n",
    "            for i in range(1, len(datetime_manifest)):\n",
    "                lower_bound = datetime_manifest[i - 1].month\n",
    "                higher_bound = datetime_manifest[i].month\n",
    "\n",
    "                match [lower_bound, higher_bound]:\n",
    "                    case [1, 4]:\n",
    "                        labels.append(\"Q1\")\n",
    "                    case [4, 7]:\n",
    "                        labels.append(\"Q2\")\n",
    "                    case [7, 10]:\n",
    "                        labels.append(\"Q3\")\n",
    "                    case [10, 1]:\n",
    "                        labels.append(\"Q4\")\n",
    "                    case _:\n",
    "                        label_mapping = pd.Series(data=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"], index=[1, 4, 7, 10])\n",
    "                        labels.append(label_mapping[lower_bound])\n",
    "        case \"month\":\n",
    "            labels = [str(month) for month in list(datetime_manifest.month)]\n",
    "            if len(labels) >= 0:\n",
    "                labels.pop()\n",
    "        case \"week\":\n",
    "            labels = [\n",
    "                \"{}\".format(\n",
    "                    datetime.strftime(datetime_manifest[i], format=\"%U\"),\n",
    "                )\n",
    "                for i in range(0, len(datetime_manifest))\n",
    "            ]\n",
    "            if len(labels) >= 0:\n",
    "                labels.pop()\n",
    "        case \"day\":\n",
    "            labels = [\"{}\".format(datetime_manifest[i].day_of_year) for i in range(0, len(datetime_manifest))]\n",
    "            if len(labels) >= 0:\n",
    "                labels.pop()\n",
    "        case _:\n",
    "            pass\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "for tuple in datetime_column_tuple:\n",
    "    # date_range reference: https://pandas.pydata.org/docs/reference/api/pandas.date_range.html\n",
    "    # Frequency reference: https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n",
    "\n",
    "    col = tuple[0]\n",
    "    datetime_columns.append(col)\n",
    "\n",
    "    # * Compare which datetime frequency is the best\n",
    "    datetime_freq_manifest = [\"year\", \"quarter\", \"month\", \"week\", \"day\"]\n",
    "    freq_manifest = [\"YS\", \"QS\", \"MS\", \"W\", \"D\"]\n",
    "    freq_mapping = pd.Series(data=freq_manifest, index=datetime_freq_manifest)\n",
    "    datetime_freq_entropy = []\n",
    "\n",
    "    for datetime_freq in datetime_freq_manifest:\n",
    "        datetime_manifest: Type[pd.DatetimeIndex] = pd.date_range(\n",
    "            start=tuple[1], end=tuple[2], freq=freq_mapping[datetime_freq]\n",
    "        )\n",
    "        datetime_manifest = datetime_manifest.union([tuple[2]])\n",
    "        labels = fill_label(datetime_freq, datetime_manifest)\n",
    "        discrete_datetime_series = pd.Series(\n",
    "            pd.cut(df[col], bins=datetime_manifest, labels=labels, include_lowest=True, ordered=False)\n",
    "        )\n",
    "\n",
    "        feature_entropy = count_feature_entropy(\n",
    "            feature_name=col, target_name=target, feature_values=discrete_datetime_series, target_values=df[target]\n",
    "        )\n",
    "        datetime_freq_entropy.append(feature_entropy)\n",
    "\n",
    "    min_value = min(datetime_freq_entropy)\n",
    "    best_index = datetime_freq_entropy.index(min_value)\n",
    "    best_freq = datetime_freq_manifest[best_index]\n",
    "\n",
    "    print(datetime_freq_manifest)\n",
    "    print(datetime_freq_entropy)\n",
    "    print(best_freq)\n",
    "\n",
    "    datetime_manifest: Type[pd.DatetimeIndex] = pd.date_range(\n",
    "        start=tuple[1], end=tuple[2], freq=freq_mapping[best_freq]\n",
    "    )\n",
    "    datetime_manifest = datetime_manifest.union([tuple[2]])\n",
    "    labels = fill_label(datetime_freq, datetime_manifest)\n",
    "\n",
    "    quantile_interval = pd.cut(df[col], bins=datetime_manifest, include_lowest=True)\n",
    "    quantile_mapping[col] = pd.Series(data=quantile_interval.tolist(), index=df[col].tolist())\n",
    "    df[col] = pd.Series(pd.cut(df[col], bins=datetime_manifest, labels=labels, include_lowest=True, ordered=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target and features handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X: pd.DataFrame\n",
    "try:\n",
    "    X = df.drop([target] + skip_features, axis=1)\n",
    "except KeyError:\n",
    "    print(\"Column of target or skip features are not exist in data frame\")\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "y = df[target].astype(\"string\")\n",
    "class_names = y.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "category_frame = X.select_dtypes(include=[\"object\", \"category\"])\n",
    "encoder = ce.OrdinalEncoder(cols=category_frame.columns)\n",
    "X = pd.DataFrame(encoder.fit_transform(X))\n",
    "\n",
    "category_column_mapping = encoder.mapping\n",
    "category_column_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "row_counts = len(X.index)\n",
    "max_depth = 30\n",
    "min_samples_split = 0\n",
    "min_samples_leaf = 0\n",
    "\n",
    "is_big_data = row_counts > 10000\n",
    "\n",
    "if is_big_data:\n",
    "    # 確保葉節點有足夠的樣本進行有意義的分析，同時避免過度細分\n",
    "    # 100 - 1000\n",
    "    min_samples_leaf = 100\n",
    "    # 確保在分割內部節點之前有足夠的樣本數\n",
    "    # 10 - 50\n",
    "    min_samples_split = 10\n",
    "else:\n",
    "    # 確保每個葉節點至少有一些樣本進行分析\n",
    "    # 1 or 2\n",
    "    min_samples_leaf = 1\n",
    "    # 確保在內部節點的樣本數較少時也可以進行分割\n",
    "    # 2 - 5\n",
    "    min_samples_split = 2\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=max_depth,\n",
    "    random_state=42,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    ")\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "feature_importance_pairs = list(zip(feature_names, feature_importance))\n",
    "feature_importance_pairs.sort(key=lambda pair: pair[1], reverse=True)\n",
    "feature_importance_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation and model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cv: k-fold, default is 5-fold\n",
    "cross_validation_score = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"交叉驗證分數:\", cross_validation_score)\n",
    "print(\"平均分數:\", cross_validation_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve decision tree path data as json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# Type definition of decision tree path data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeNode:\n",
    "    id: int\n",
    "    labels: list[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeEdge:\n",
    "    id: int\n",
    "    label: str\n",
    "    head: int\n",
    "    tail: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeGraph:\n",
    "    nodes: list[DecisionTreeNode]\n",
    "    edges: dict[str, DecisionTreeEdge]  # node1_node2 as key value\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreePath:\n",
    "    path: list[int]\n",
    "    nodeLabel: dict[int, list[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "\n",
    "out_file_path = f\"{Path.cwd().absolute()}/temp/temp.dot\"\n",
    "\n",
    "# * Scikit-learn decision tree:\n",
    "# Using optimized version of the CART algorithm\n",
    "# Not support categorical variable for now, that is, categorical variable need to encode\n",
    "\n",
    "# * Entropy range:\n",
    "# From 0 to 1 for binary classification (target has only two classes, true or false)\n",
    "# From 0 to log base 2 k where k is the number of classes\n",
    "\n",
    "class_names = clf.classes_.tolist()\n",
    "\n",
    "dot_file = tree.export_graphviz(\n",
    "    clf,\n",
    "    out_file=out_file_path,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    max_depth=max_depth,\n",
    "    label=\"all\",\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "with open(out_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dot_file = f.read()\n",
    "\n",
    "# Use graphviz lib to convert dot format to json format\n",
    "source = Source(dot_file)\n",
    "json_graph = source.pipe(format=\"json\").decode(\"utf-8\")\n",
    "dict_graph: dict = json.loads(json_graph)\n",
    "\n",
    "# Filter needed part\n",
    "nodes = list(\n",
    "    map(\n",
    "        lambda o: {\"id\": o.get(\"_gvid\"), \"labels\": o.get(\"label\").split(\"\\\\n\")},\n",
    "        dict_graph.get(\"objects\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "edges = dict(\n",
    "    map(\n",
    "        lambda o: (\n",
    "            str(o.get(\"tail\")) + \"_\" + str(o.get(\"head\")),\n",
    "            {\n",
    "                \"id\": o.get(\"_gvid\"),\n",
    "                \"label\": o.get(\"headlabel\"),\n",
    "                \"head\": o.get(\"tail\"),\n",
    "                \"tail\": o.get(\"head\"),\n",
    "            },\n",
    "        ),\n",
    "        dict_graph.get(\"edges\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan\n",
    "\n",
    "data_information: dict[str, str or list or dict] = {}\n",
    "data_information[\"target_name\"] = target\n",
    "data_information[\"target_values\"] = class_names\n",
    "data_information[\"feature_names\"] = feature_names\n",
    "\n",
    "# Numeric\n",
    "feature_values: dict[str, dict[str, str or list]] = {}\n",
    "\n",
    "for n in numerical_column_tuple:\n",
    "    feature_values[n[0]] = {\"type\": \"numeric\", \"value\": [n[1], n[2]]}\n",
    "\n",
    "# Datetime\n",
    "for part_df in datetime_column_tuple:\n",
    "    format = \"%Y-%m-%d %X\"\n",
    "    feature_values[part_df[0]] = {\n",
    "        \"type\": \"datetime\",\n",
    "        \"value\": [part_df[1].strftime(format), part_df[2].strftime(format)],\n",
    "    }\n",
    "\n",
    "# Category\n",
    "for c in category_column_mapping:\n",
    "    is_datetime_column = c[\"col\"] in datetime_columns\n",
    "    feature_values[c[\"col\"]] = {\n",
    "        \"type\": \"datetime\" if is_datetime_column else \"category\",\n",
    "        \"value\": (c[\"mapping\"].index.to_list()),\n",
    "        \"mapping\": pd.Series(dict((v, k) for k, v in c[\"mapping\"].items())),\n",
    "    }\n",
    "    feature_values[c[\"col\"]][\"value\"].pop()\n",
    "\n",
    "unstored_features = list(set(feature_names) - set(list(feature_values.keys())))\n",
    "\n",
    "for f in unstored_features:\n",
    "    split_value = df[f].unique().astype(\"str\").tolist()\n",
    "    df[f] = df[f].astype(\"str\")\n",
    "\n",
    "    mapping_pairs = dict((i + 1, split_value[i]) for i in range(len(split_value)))\n",
    "    mapping_pairs[-2] = nan\n",
    "    mapping = pd.Series(mapping_pairs)\n",
    "    feature_values[f] = {\n",
    "        \"type\": \"category\",\n",
    "        \"value\": split_value,\n",
    "        \"mapping\": mapping,\n",
    "    }\n",
    "\n",
    "data_information[\"feature_values\"] = feature_values\n",
    "data_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree path resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "def DecisionTreePathResolver(graph: DecisionTreeGraph, root_id: int = 0):\n",
    "    paths: list[DecisionTreePath] = []\n",
    "\n",
    "    # DFS: Depth-First Search\n",
    "    def SearchPathByDFS(current_id: int = 0, path: list[int] = []):\n",
    "        if not graph:\n",
    "            return\n",
    "\n",
    "        path.append(current_id)\n",
    "\n",
    "        edge_values = list(map(lambda edge: DecisionTreeEdge(**edge), list(graph.edges.values())))\n",
    "        outgoing_edges = list(filter(lambda edge: edge.head == current_id, edge_values))\n",
    "\n",
    "        # 如果目前節點沒有出邊（即為最底層節點），將路徑加入結果中\n",
    "        if len(outgoing_edges) == 0:\n",
    "            last_id = path[len(path) - 1]\n",
    "            last_node = DecisionTreeNode(**(graph.nodes[last_id]))\n",
    "            node_labels: dict[int, list[str]] = {}\n",
    "\n",
    "            # ! 排除 entropy 高的 path\n",
    "            entropy = float(last_node.labels[0].split(\" \")[2])\n",
    "\n",
    "            if entropy > log(len(feature_names), 2) / 2:\n",
    "                path.pop()\n",
    "                return\n",
    "            # ! ####################\n",
    "\n",
    "            node_labels[last_id] = last_node.labels\n",
    "\n",
    "            for i in range(0, len(path) - 1):\n",
    "                node_id = path[i]\n",
    "                labels = DecisionTreeNode(**(graph.nodes[node_id])).labels\n",
    "\n",
    "                # 如果下一個的 node id 是上一個 +1 則是 true，不然的話是 false\n",
    "                # * left edge <= => true\n",
    "                # * right edge > => false\n",
    "\n",
    "                next_id = path[i + 1]\n",
    "\n",
    "                if node_id + 1 != next_id:\n",
    "                    new_labels = [*labels]\n",
    "                    condition = new_labels[0]\n",
    "                    split_condition = condition.split(\" \")\n",
    "                    split_condition[1] = \">\"\n",
    "                    new_labels[0] = \" \".join(split_condition)\n",
    "                    node_labels[node_id] = new_labels\n",
    "                    continue\n",
    "\n",
    "                node_labels[node_id] = [*labels]\n",
    "\n",
    "            paths.append(DecisionTreePath([*path], node_labels))\n",
    "\n",
    "        # 遍歷目前節點的所有出邊\n",
    "        else:\n",
    "            for edge in outgoing_edges:\n",
    "                next_id = edge.tail\n",
    "\n",
    "                # 遞迴呼叫深度優先搜索\n",
    "                SearchPathByDFS(next_id, path)\n",
    "\n",
    "        # 回溯，從路徑中移除目前節點\n",
    "        path.pop()\n",
    "\n",
    "    SearchPathByDFS(root_id)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_graph = DecisionTreeGraph(nodes, edges)\n",
    "paths = DecisionTreePathResolver(decision_tree_graph, 0)\n",
    "print(\"Path counts = {}\".format(len(paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree path analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "\n",
    "def DecisionTreePathAnalyzer(paths: list[DecisionTreePath], target_values: list[str], feature_names: list[str]):\n",
    "    path_analysis_result: dict = {}\n",
    "    for split_value in target_values:\n",
    "        path_analysis_result[split_value] = []\n",
    "\n",
    "    for path in paths:\n",
    "        path_analysis_result_part = {}\n",
    "\n",
    "        for feature_name in feature_names:\n",
    "            path_analysis_result_part[feature_name] = data_information[\"feature_values\"][feature_name][\"value\"].copy()\n",
    "\n",
    "        for node_id in path.path:\n",
    "            labels = path.nodeLabel[node_id][0].split(\" \")\n",
    "            feature_name = labels[0]\n",
    "            split_symbol = labels[1]\n",
    "            split_value = float(labels[2])\n",
    "\n",
    "            if node_id == path.path[len(path.path) - 1]:\n",
    "                class_name = path.nodeLabel[node_id][3].split(\" \")[2]\n",
    "\n",
    "                sample_value = \" \".join(path.nodeLabel[node_id][2].split(\" \")[2:]).split(\", \")\n",
    "                sample_value[0] = sample_value[0][1:]\n",
    "                sample_value[len(sample_value) - 1] = sample_value[len(sample_value) - 1][0:-1]\n",
    "\n",
    "                path_analysis_result_part[\"entropy\"] = float(split_value)\n",
    "                path_analysis_result_part[\"samples\"] = list(map(lambda value: int(value), sample_value))\n",
    "                path_analysis_result_part[\"labels\"] = target_values\n",
    "                path_analysis_result_part[\"class\"] = class_name\n",
    "                path_analysis_result_part[\"target\"] = target\n",
    "\n",
    "                path_analysis_result[class_name].append(path_analysis_result_part)\n",
    "                break\n",
    "\n",
    "            feature_type = data_information[\"feature_values\"][labels[0]][\"type\"]\n",
    "            split_situation = [split_symbol, feature_type]\n",
    "            mapping: pd.Series = data_information[\"feature_values\"][feature_name][\"mapping\"]\n",
    "\n",
    "            match split_situation:\n",
    "                case [\"<=\", \"category\"]:\n",
    "                    index = path_analysis_result_part[feature_name].index(mapping[floor(split_value)])\n",
    "                    path_analysis_result_part[feature_name] = path_analysis_result_part[feature_name][0 : index + 1]\n",
    "                case [\"<=\", \"datetime\"]:\n",
    "                    index = path_analysis_result_part[feature_name].index(mapping[floor(split_value)])\n",
    "                    path_analysis_result_part[feature_name] = path_analysis_result_part[feature_name][0 : index + 1]\n",
    "                case [\">\", \"category\"]:\n",
    "                    index = path_analysis_result_part[feature_name].index(mapping[ceil(split_value)])\n",
    "                    path_analysis_result_part[feature_name] = path_analysis_result_part[feature_name][index:]\n",
    "                case [\">\", \"datetime\"]:\n",
    "                    index = path_analysis_result_part[feature_name].index(mapping[ceil(split_value)])\n",
    "                    path_analysis_result_part[feature_name] = path_analysis_result_part[feature_name][index:]\n",
    "                case _:\n",
    "                    print(\"no match case\")\n",
    "\n",
    "    return path_analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_analysis_result = DecisionTreePathAnalyzer(paths=paths, target_values=class_names, feature_names=feature_names)\n",
    "path_analysis_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert path analysis result to json string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.dumps(path_analysis_result)\n",
    "json_object = json.loads(json_str)\n",
    "\n",
    "for k in json_object:\n",
    "    print(k, len(json_object[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "target_class = random.choice(class_names)\n",
    "paths = path_analysis_result[target_class]\n",
    "print(f\"{target_class} path count: {len(paths)}\")\n",
    "target_path = random.choice(paths)\n",
    "\n",
    "part_df = df.copy()\n",
    "\n",
    "for feature in feature_names:\n",
    "    value_df = pd.DataFrame(columns=df.columns)\n",
    "    for value in target_path[feature]:\n",
    "        value_df = pd.concat([value_df, part_df.loc[(part_df[feature] == value)]], ignore_index=False)\n",
    "    part_df = value_df\n",
    "\n",
    "print(f\"Target column of analysis: {target_path['target']}\")\n",
    "print(f\"Most class of this path: {target_path['class']}\")\n",
    "print(f\"Time range: {datetime_column_tuple[0][1]} to {datetime_column_tuple[0][2]}\")\n",
    "print(part_df[target].value_counts())\n",
    "\n",
    "part_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
